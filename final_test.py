import cv2
import mediapipe as mp
import numpy as np
import time
import torch
import torch.nn as nn
from configs.settings import Config
from PIL import Image, ImageDraw, ImageFont

# ì‚¬ìš©í•  í•œê¸€ í°íŠ¸ ê²½ë¡œ (AppleSDGothicNeo)
FONT_PATH = "/System/Library/Fonts/Supplemental/AppleSDGothicNeo.ttc"

def put_text(img, text, pos, font_path, font_size=32, color=(0, 0, 255)):

    # OpenCV BGR ì´ë¯¸ì§€ë¥¼ PIL RGB ì´ë¯¸ì§€ë¡œ ë³€í™˜
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pil_img = Image.fromarray(img_rgb)
    draw = ImageDraw.Draw(pil_img)
    font = ImageFont.truetype(font_path, font_size)
    draw.text(pos, text, font=font, fill=(color[2], color[1], color[0]))  # PILì€ RGB ìˆœì„œ ì‚¬ìš©
    # ë‹¤ì‹œ OpenCV ì´ë¯¸ì§€(BGR)ë¡œ ë³€í™˜
    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)

# ----- ëª¨ë¸ ë° í´ë˜ìŠ¤ ì •ì˜  -----
class SignLanguageModel(nn.Module):
    def __init__(self, config: Config):
        super(SignLanguageModel, self).__init__()
        self.config = config

        # ì…ë ¥ ì²˜ë¦¬: [B, 30, 126] -> [B, 30, 21, 6]
        self.input_processor = nn.Sequential(
            nn.Unflatten(2, (21, 6)),
            nn.Dropout(0.1)
        )

        # ë©”ëª¨ë¦¬ íš¨ìœ¨í˜• CNN
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=3, padding=1),
            nn.BatchNorm2d(8),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(8, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((5, 3))
        )

        # ì–‘ë°©í–¥ GRU
        self.temporal_encoder = nn.GRU(
            input_size=16 * 5 * 3,
            hidden_size=32,
            bidirectional=True,
            batch_first=True
        )

        # ì´ˆê²½ëŸ‰ ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, config.NUM_CLASSES)
        )

    def forward(self, x):
        # x: [B, 30, 126] â†’ [B, 30, 21, 6] í›„ ì¶”ê°€ ì°¨ì› ì‚½ì…: [B, 30, 1, 21, 6]
        x = self.input_processor(x).unsqueeze(2)
        batch_size, timesteps = x.size(0), x.size(1)
        # CNN ì—°ì‚°ì„ ìœ„í•´ ë°°ì¹˜ì™€ íƒ€ì„ìŠ¤í… ê²°í•©: [B*30, 1, 21, 6]
        x = x.view(-1, 1, 21, 6)
        features = self.cnn(x).view(batch_size, timesteps, -1)
        _, h_n = self.temporal_encoder(features)
        last_output = torch.cat([h_n[-2], h_n[-1]], dim=1)  # [B, 64]
        return self.classifier(last_output)

# config ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë¸ ìƒì„± ì‹œ ì¸ìë¡œ ì „ë‹¬
config = Config()

# í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í´ë˜ìŠ¤(ì œìŠ¤ì²˜) ì„¤ì •
GESTURES = {idx: class_name for idx, class_name in enumerate(config.CLASSES)}
#GESTURES = { 29:'ì‹¤íŒ¨'}

model = SignLanguageModel(config)
model.load_state_dict(torch.load("models/saved_models/train_480_best_model.pth", map_location=torch.device('cpu')))
model.eval()  # í‰ê°€ ëª¨ë“œ

# ----- Mediapipe ì…‹ì—… -----
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
# ë‘ ì† ëª¨ë‘ ê²€ì¶œí•˜ë„ë¡ ì„¤ì •
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5)

# ğŸ”¥ ë²„í¼ ì‚¬ì´ì¦ˆ ì„¤ì •ì´ ì¶”ê°€ëœ ì›¹ìº  ì´ˆê¸°í™”
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ í¬ê¸° 1ë¡œ ì„¤ì •

# ìƒíƒœ ê´€ë¦¬ ë³€ìˆ˜ë“¤
phase = "countdown"
phase_start_time = time.time()
captured_frames = []
current_problem_idx = 0
attempts_left = 3
total_results = []
PROBLEMS = list(GESTURES.values())

while True:
    ret, frame = cap.read()
    if not ret:
        print("í”„ë ˆì„ íšë“ ì‹¤íŒ¨! ì¬ì‹œë„...")
        cap.release()
        time.sleep(1)
        cap = cv2.VideoCapture(0)
        continue

    # ë¯¸ë””ì–´íŒŒì´í”„ ì²˜ë¦¬
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(frame_rgb)

    # í™”ë©´ í‘œì‹œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
    if results.multi_hand_landmarks and results.multi_handedness:
        for idx, (hand_landmarks, handedness) in enumerate(zip(results.multi_hand_landmarks,
                                                               results.multi_handedness)):
            label = handedness.classification[0].label
            frame = put_text(frame, f"Hand {idx + 1}: {label}", (10, 50 + idx * 30),
                             FONT_PATH, font_size=24, color=(0, 255, 0))
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    current_time = time.time()

    # ğŸ”¥ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ -------------------------------------------------
    if phase == "countdown":
        elapsed = current_time - phase_start_time
        count = int(3 - elapsed)
        if count > 0:
            frame = put_text(frame,
                             f"[ë¬¸ì œ {current_problem_idx + 1}/{len(PROBLEMS)}] ì¤€ë¹„ì‹œê°„: {count}ì´ˆ\nì •ë‹µ: {PROBLEMS[current_problem_idx]}",
                             (50, 50), FONT_PATH, 32, (255, 255, 255))
        else:
            print(f"\nâ–¼â–¼â–¼ ìƒˆ ë¬¸ì œ ì‹œì‘: {PROBLEMS[current_problem_idx]} â–¼â–¼â–¼")
            phase = "capture"
            captured_frames = []
            phase_start_time = current_time  # ğŸ”¥ ìº¡ì²˜ ì‹œì‘ ì‹œê°„ ì´ˆê¸°í™”

    elif phase == "capture":
        frame = put_text(frame, "ë°ì´í„° ìˆ˜ì§‘ ì¤‘...", (50, 50),
                         FONT_PATH, font_size=32, color=(255, 255, 255))

        # ğŸ”¥ í•­ìƒ í”„ë ˆì„ ë°ì´í„° ì €ì¥ (ì†ì´ ì—†ì–´ë„ 0ìœ¼ë¡œ ì±„ì›€)
        landmarks = np.zeros(126)
        if results.multi_hand_landmarks:
            temp_landmarks = []
            for hand_landmarks in results.multi_hand_landmarks:
                temp_landmarks.extend([lm.x for lm in hand_landmarks.landmark])
                temp_landmarks.extend([lm.y for lm in hand_landmarks.landmark])
                temp_landmarks.extend([lm.z for lm in hand_landmarks.landmark])
            temp_landmarks = np.array(temp_landmarks[:126])
            landmarks[:len(temp_landmarks)] = temp_landmarks

        captured_frames.append(landmarks.tolist())
        print(f"â†’ ìˆ˜ì§‘ í”„ë ˆì„: {len(captured_frames)}/30", end='\r')  # ì§„í–‰ìƒí™© í‘œì‹œ

        # ğŸ”¥ 30í”„ë ˆì„ ë„ë‹¬ ì²´í¬ (ì¡°ê±´ë¬¸ ìœ„ì¹˜ ë³€ê²½)
        if len(captured_frames) >= 30:
            print("\nâœ“ 30í”„ë ˆì„ ìˆ˜ì§‘ ì™„ë£Œ â†’ ì˜ˆì¸¡ ìˆ˜í–‰")
            input_data = np.array(captured_frames).reshape(1, 30, 126)
            with torch.no_grad():
                output = model(torch.tensor(input_data, dtype=torch.float32))
                pred_class = torch.argmax(output).item()

            prediction = GESTURES.get(pred_class, "Unknown")
            total_results.append({
                "problem": PROBLEMS[current_problem_idx],
                "attempt": 3 - attempts_left + 1,
                "prediction": prediction
            })
            phase = "display"
            phase_start_time = current_time
            attempts_left -= 1

    elif phase == "display":
        current_result = total_results[-1]
        text = f"ì‹œë„ {current_result['attempt']}ë²ˆ: {current_result['prediction']}"
        frame = put_text(frame, text, (50, 150), FONT_PATH, 40, (0, 0, 255))

        if current_time - phase_start_time >= 1:
            if attempts_left > 0:
                phase = "countdown"
                phase_start_time = current_time
            else:
                current_problem_idx += 1
                if current_problem_idx >= len(PROBLEMS):
                    break
                attempts_left = 3
                phase = "countdown"
                phase_start_time = current_time

    cv2.imshow("Webcam Feed", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥
print("\n===== ìµœì¢… ê²°ê³¼ =====")
for idx, problem in enumerate(PROBLEMS):
    problem_results = [r for r in total_results if r['problem'] == problem]
    correct_count = sum(1 for r in problem_results if r['prediction'] == problem)
    print(f"ë¬¸ì œ {idx + 1}: {problem} | ì‹œë„íšŸìˆ˜: {len(problem_results)} | ì •ë‹µíšŸìˆ˜: {correct_count}")

cap.release()
cv2.destroyAllWindows()
